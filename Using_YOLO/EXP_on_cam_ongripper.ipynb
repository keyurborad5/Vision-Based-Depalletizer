{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Making Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package           Version\n",
      "----------------- -----------\n",
      "backcall          0.2.0\n",
      "colorama          0.4.4\n",
      "debugpy           1.5.1\n",
      "decorator         5.1.0\n",
      "entrypoints       0.3\n",
      "ipykernel         6.6.0\n",
      "ipython           7.30.1\n",
      "jedi              0.18.1\n",
      "jupyter-client    7.1.0\n",
      "jupyter-core      4.9.1\n",
      "keyboard          0.13.5\n",
      "matplotlib-inline 0.1.3\n",
      "nest-asyncio      1.5.4\n",
      "numpy             1.21.5\n",
      "opencv-python     4.5.4.60\n",
      "parso             0.8.3\n",
      "pickleshare       0.7.5\n",
      "pip               21.1.3\n",
      "prompt-toolkit    3.0.24\n",
      "Pygments          2.10.0\n",
      "pylogix           0.7.16\n",
      "pyrealsense2      2.50.0.3812\n",
      "python-dateutil   2.8.2\n",
      "pywin32           303\n",
      "pyzmq             22.3.0\n",
      "setuptools        56.0.0\n",
      "six               1.16.0\n",
      "tornado           6.1\n",
      "traitlets         5.1.1\n",
      "wcwidth           0.2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pyrealsense2\n",
    "\n",
    "from realsense_aligned_depth import *  #note: I have changed the frame size to 848X480 from 640X480\n",
    "\n",
    "\n",
    "\n",
    "def empty(a):\n",
    "    pass\n",
    "\n",
    "#for displaying trackbar to adjust values of kernal,threshold and min area of contour\n",
    "cv2.namedWindow(\"Parameter\")\n",
    "cv2.resizeWindow(\"Parameter\",640,240)\n",
    "cv2.createTrackbar(\"kernel\",\"Parameter\",7,50,empty)\n",
    "cv2.createTrackbar(\"Threshold\",\"Parameter\",75,254,empty)\n",
    "cv2.createTrackbar(\"AREA\",\"Parameter\",250,9000,empty)\n",
    "\n",
    "# Initialize Camera Intel Realsense\n",
    "dc = DepthCamera()\n",
    "\n",
    "#capturing backgroung image\n",
    "first_frame= True\n",
    "\n",
    "while first_frame is True:\n",
    "    ret, depth_bg_frame, color_bg_frame = dc.get_frame()\n",
    "    bg_bilateral = cv2.bilateralFilter(color_bg_frame, 15, 75, 75)\n",
    "    bg_gray = cv2.cvtColor(bg_bilateral, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow(\"showing Video for bg\",bg_gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('c'):\n",
    "        ff=bg_gray\n",
    "        #cv2.imwrite(\"c1.png\",bg_gray)\n",
    "        first_frame= False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create mouse event\n",
    "#cv2.namedWindow(\"Color frame\")\n",
    "#cv2.setMouseCallback(\"Color frame\", show_distance)\n",
    "\n",
    "while True:\n",
    "    ret, depth_frame, color_frame = dc.get_frame()\n",
    "    if ret is True:\n",
    "        bilateral = cv2.bilateralFilter(color_frame, 15, 60, 60)\n",
    "        gray = cv2.cvtColor(bilateral, cv2.COLOR_BGR2GRAY)\n",
    "        image_sub = cv2.absdiff(ff, gray)\n",
    "        #kernel=cv2.getTrackbarPos(\"kernel\", \"Parameter\")#for trackbar\n",
    "        #if (kernel%2) ==0:\n",
    "        #    kernel=kernel+1\n",
    "        #else:\n",
    "        #    pass\n",
    "        #blur=cv2.medianBlur(gray,kernel)\n",
    "        thres=cv2.getTrackbarPos(\"Threshold\",\"Parameter\")#for trackbar\n",
    "        retu, th1 = cv2.threshold(image_sub,thres,255,cv2.THRESH_BINARY)\n",
    "        contours, hierarchy = cv2.findContours(th1, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(color_frame, contours, -1, (0,255,0), 1)\n",
    "        for i in range (len(contours)):\n",
    "            cnt2 = contours[i]\n",
    "            area=cv2.contourArea(cnt2)\n",
    "            AREA = cv2.getTrackbarPos(\"AREA\", \"Parameter\")#for trackbar\n",
    "            if area>=AREA:\n",
    "                #epsilon = 0.1*cv2.arcLength(cnt2,True)\n",
    "                #approx = cv2.approxPolyDP(cnt2,epsilon,True)\n",
    "                rect = cv2.minAreaRect(cnt2) #returns [rectangle center(x, y), (width, height), rotation angle)].\n",
    "                box = cv2.boxPoints(rect)\n",
    "                box = np.int0(box)\n",
    "                color_frame = cv2.drawContours(color_frame,[box],0,(0,255,255),2)\n",
    "                #M = cv2.moments(cnt2)\n",
    "                #cx2 = int(M['m10']/M['m00'])\n",
    "                #cy2 = int(M['m01']/M['m00'])\n",
    "                cx2=int(rect[0][0])\n",
    "                cy2=int(rect[0][1])\n",
    "                point = (cx2 , cy2)\n",
    "                \n",
    "                width= int(rect[1][0])\n",
    "                height= int(rect[1][1])\n",
    "                \n",
    "                 # Show distance for a specific point\n",
    "                cv2.circle(color_frame, point, 4, (0, 0, 255))\n",
    "                distance =depth_frame[int(point[1]),int(point[0])] #depth_frame[240,320] #\n",
    "                \n",
    "                dist=(distance)/10          #input in cm\n",
    "                cv2.putText(color_frame, \"{}cm\".format(dist), (point[0], point[1] - 20), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2)\n",
    "        \n",
    "                #cvtfac=0.00304535*distance**(0.921814) -0.17026#1#0.00167228*distance -0.00381802\n",
    "                cvtfac =1#0.00325729*(distance)**(0.914301)-0.144273\n",
    "                #Rx=(round(width*cvtfac,2))\n",
    "                #H8\n",
    "                #Ry=(round(height*cvtfac,2))\n",
    "                #print(width,height)\n",
    "                \n",
    "                #Pixel_to_CM\n",
    "                \n",
    "                Rx=(round(cx2*cvtfac,2))\n",
    "                Ry=(round(cy2*cvtfac,2))\n",
    "                \n",
    "                \n",
    "                #color_frame=cv2.circle(color_frame, (cx2, cy2), 7, (0, 100, 200), -1)\n",
    "                angle=rect[-1]\n",
    "                #print(angle)\n",
    "                if angle<-45:\n",
    "                    angle=-(90+angle)\n",
    "                else:\n",
    "                    angle=-angle\n",
    "                #print(angle,\"deg\")\n",
    "                x_dis=abs(cx2-box[0,0]) #dist btw x value of center of contour and x-value of first vertex of contour\n",
    "                y_dis=abs(cy2-box[0,1])#dist btw y value of center of contour and y-value of first vertex of contour\n",
    "                \n",
    "                ###########################################\n",
    "                #below values are not used because we change the way to detect orientation\n",
    "                #a=np.array((box[0,0],box[0,1]))\n",
    "                #b=np.array((box[1,0],box[1,1]))\n",
    "                #c=np.array((box[2,0],box[2,1]))\n",
    "                #ab=np.linalg.norm(a-b)\n",
    "                #bc=np.linalg.norm(b-c)\n",
    "                #print(ab,bc)\n",
    "                \n",
    "                #if (ab<bc and (angle<=0 and angle>-10)) or (ab>bc and (angle>0 and angle<10)):\n",
    "                 #   frame=cv2.putText(frame,\"Vertical\" , (cx2-10, cy2-10), cv2.FONT_HERSHEY_SIMPLEX,0.5, (0, 0, 155), 2)\n",
    "                    #print(\"its verticle\")\n",
    "                #elif (ab>bc and (angle<=0 and angle>-10)) or (ab<bc and (angle>0 and angle<10)):\n",
    "                    #print(\"its Horizontal\")\n",
    "                 #   frame=cv2.putText(frame,\"Horizontal\" , (cx2-10, cy2-10), cv2.FONT_HERSHEY_SIMPLEX,0.5, (0, 155, 0), 2)\n",
    "                #############################################\n",
    "                \n",
    "               \n",
    "                if (x_dis<y_dis and (angle<10 and angle>-10)):\n",
    "                    color_frame=cv2.putText(color_frame,\"v\"+str( Rx)+\",\"+str( Ry) , (cx2-10, cy2-10), cv2.FONT_HERSHEY_SIMPLEX,0.5, (0, 0, 155), 2)\n",
    "                    #print(\"its verticle\")\n",
    "                elif (x_dis>y_dis and (angle<10 and angle>-10)):\n",
    "                    #print(\"its Horizontal\")\n",
    "                    color_frame=cv2.putText(color_frame,\"H\"+str(Rx)+\",\"+str(Ry), (cx2-10, cy2-10), cv2.FONT_HERSHEY_SIMPLEX,0.5, (0, 155, 0), 2)\n",
    "                else:\n",
    "                    color_frame=cv2.putText(color_frame,\"I\"+str(Rx)+\",\"+str(Ry) , (cx2-10, cy2-10), cv2.FONT_HERSHEY_SIMPLEX,0.5, (155, 0, 0), 2)\n",
    "                    #print(\"Its inclined\")\n",
    "\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "\n",
    "    #cv2.imshow(\"depth frame\", depth_frame)\n",
    "    cv2.imshow(\"Color frame\", color_frame)\n",
    "    #cv2.imshow(\"gray frame\", gray)\n",
    "    #cv2.imshow(\"blur frame\", blur)\n",
    "    #cv2.imshow(\"th1 frame\", th1)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        dc.release()\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Depalletizer Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE SYSTEM IS ON _ press\n",
      "waiting for robot to go to home position\n",
      "Reached home position robot waiting\n",
      "[962, 959, 963, 967]\n",
      "4\n",
      "962.75\n",
      "moving robot in z direction : -1567.25\n",
      "Moved downwards, now waiting for carton detection\n",
      "[1045, 1049, 1048, 1045, 1053, 1040, 1049, 1042, 1046, 1023, 1048, 1045, 1035, 1053]\n",
      "14\n",
      "1044.357142857143\n",
      "Objects Detection took 0.50087 seconds\n",
      "Object 1: carton\n",
      "Object 2: carton\n",
      "Object 3: carton\n",
      "Object 4: carton\n",
      "Object 5: carton\n",
      "Object 6: carton\n",
      "[[-248.0, 153.0, -552.8928571428571, 0, 0, 0], [72.0, 116.0, -552.8928571428571, 0, 0, 900], [-215.0, -174.0, -552.8928571428571, 0, 0, 900], [97.0, -198.0, -552.8928571428571, 0, 0, 0], [407.0, 138.0, -552.8928571428571, 0, 0, 0], [432.0, -182.0, -552.8928571428571, 0, 0, 900]]\n",
      "\n",
      "Total objects been detected: 45\n",
      "Number of objects left after non-maximum suppression: 6\n",
      "[[-248.0, 153.0, -552.8928571428571, 0, 0, 0], [-215.0, -174.0, -552.8928571428571, 0, 0, 900], [72.0, 116.0, -552.8928571428571, 0, 0, 900], [97.0, -198.0, -552.8928571428571, 0, 0, 0], [407.0, 138.0, -552.8928571428571, 0, 0, 0], [432.0, -182.0, -552.8928571428571, 0, 0, 900]]\n",
      "False\n",
      "Grab item from x:153.0, y:-248.0, z:-552.8928571428571, Rz:0\n",
      "False\n",
      "Grab item from x:-174.0, y:-215.0, z:-552.8928571428571, Rz:900\n",
      "False\n",
      "Grab item from x:116.0, y:72.0, z:-552.8928571428571, Rz:900\n",
      "False\n",
      "Grab item from x:-198.0, y:97.0, z:-552.8928571428571, Rz:0\n",
      "False\n",
      "Grab item from x:138.0, y:407.0, z:-552.8928571428571, Rz:0\n",
      "False\n",
      "Grab item from x:-182.0, y:432.0, z:-552.8928571428571, Rz:900\n",
      "exited inner loop\n",
      "waiting for robot to go to home position\n",
      "Reached home position robot waiting\n",
      "[1126, 1114, 1107, 1115]\n",
      "4\n",
      "1115.5\n",
      "moving robot in z direction : -1414.5\n",
      "Moved downwards, now waiting for carton detection\n",
      "[1084, 1011, 1087, 1067, 1016, 1087, 1082, 1081, 1084, 1010, 1022, 1020, 1086, 1025]\n",
      "14\n",
      "1054.4285714285713\n",
      "Objects Detection took 0.56430 seconds\n",
      "Object 1: carton\n",
      "Object 2: carton\n",
      "Object 3: carton\n",
      "Object 4: carton\n",
      "[[418.0, 110.0, -390.07142857142867, 0, 0, 900], [65.0, -187.0, -390.07142857142867, 0, 0, 900], [383.0, -231.0, -390.07142857142867, 0, 0, 0], [98.0, 136.0, -390.07142857142867, 0, 0, 0]]\n",
      "\n",
      "Total objects been detected: 34\n",
      "Number of objects left after non-maximum suppression: 4\n",
      "[[65.0, -187.0, -390.07142857142867, 0, 0, 900], [98.0, 136.0, -390.07142857142867, 0, 0, 0], [383.0, -231.0, -390.07142857142867, 0, 0, 0], [418.0, 110.0, -390.07142857142867, 0, 0, 900]]\n",
      "False\n",
      "Grab item from x:-187.0, y:65.0, z:-390.07142857142867, Rz:900\n",
      "False\n",
      "Grab item from x:136.0, y:98.0, z:-390.07142857142867, Rz:0\n",
      "False\n",
      "Grab item from x:-231.0, y:383.0, z:-390.07142857142867, Rz:0\n",
      "False\n",
      "Grab item from x:110.0, y:418.0, z:-390.07142857142867, Rz:900\n",
      "exited inner loop\n",
      "waiting for robot to go to home position\n",
      "Reached home position robot waiting\n",
      "[1108, 1147, 1139, 1139]\n",
      "4\n",
      "1133.25\n",
      "moving robot in z direction : -1396.75\n",
      "Moved downwards, now waiting for carton detection\n",
      "[1222, 1242, 1246, 1251, 1244, 1251, 1244, 1233, 1246, 1244, 1244, 1248, 1246, 1248]\n",
      "14\n",
      "1243.5\n",
      "Objects Detection took 0.57628 seconds\n",
      "Object 1: carton\n",
      "Object 2: carton\n",
      "Object 3: carton\n",
      "Object 4: carton\n",
      "Object 5: carton\n",
      "Object 6: carton\n",
      "[[-237.0, -163.0, -183.25, 0, 0, 900], [-240.0, 135.0, -183.25, 0, 0, 0], [78.0, -217.0, -183.25, 0, 0, 0], [416.0, -207.0, -183.25, 0, 0, 900], [79.0, 102.0, -183.25, 0, 0, 900], [410.0, 121.0, -183.25, 0, 0, 0]]\n",
      "\n",
      "Total objects been detected: 53\n",
      "Number of objects left after non-maximum suppression: 6\n",
      "[[-240.0, 135.0, -183.25, 0, 0, 0], [-237.0, -163.0, -183.25, 0, 0, 900], [78.0, -217.0, -183.25, 0, 0, 0], [79.0, 102.0, -183.25, 0, 0, 900], [410.0, 121.0, -183.25, 0, 0, 0], [416.0, -207.0, -183.25, 0, 0, 900]]\n",
      "False\n",
      "Grab item from x:135.0, y:-240.0, z:-183.25, Rz:0\n",
      "False\n",
      "Grab item from x:-163.0, y:-237.0, z:-183.25, Rz:900\n",
      "False\n",
      "Grab item from x:-217.0, y:78.0, z:-183.25, Rz:0\n",
      "False\n",
      "Grab item from x:102.0, y:79.0, z:-183.25, Rz:900\n",
      "False\n",
      "Grab item from x:121.0, y:410.0, z:-183.25, Rz:0\n",
      "False\n",
      "Grab item from x:-207.0, y:416.0, z:-183.25, Rz:900\n",
      "exited inner loop\n",
      "waiting for robot to go to home position\n",
      "Reached home position robot waiting\n",
      "[1520, 1523, 1520, 1517]\n",
      "4\n",
      "1520.0\n",
      "moving robot in z direction : -1010.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pyrealsense2\n",
    "import numpy as np\n",
    "from pylogix import PLC\n",
    "import keyboard\n",
    "import time\n",
    "from realsense_aligned_depth import *\n",
    "\n",
    "def Sort(s):\n",
    "  \n",
    "    # reverse = None (Sorts in Ascending order)\n",
    "    # key is set to sort using second element of \n",
    "    # sublist lambda has been used\n",
    "    s.sort(key = lambda x: x[0])\n",
    "    return s\n",
    "print(\"THE SYSTEM IS ON _ press\")\n",
    "\n",
    "while True:\n",
    "    \n",
    "    comm =PLC()\n",
    "    comm.IPAddress = '192.168.255.2'\n",
    "    comm.Write('signal_to_rob_02',0)\n",
    "    comm.Write('signal_to_rob_04',0)\n",
    "    \n",
    "    comm.Write('signal_to_rob_06',0)\n",
    "    no_carton=0\n",
    "    print(\"waiting for robot to go to home position\")\n",
    "\n",
    "    while True: #should be true only in the presence of pallet\n",
    "        sig_1=comm.Read('signal_to_pc_03')\n",
    "        \n",
    "        # if keyboard.is_pressed('p'):\n",
    "        #     break\n",
    "        if sig_1.Value is True:\n",
    "            break\n",
    "    print(\"Reached home position robot waiting\")\n",
    "    \n",
    "    \n",
    "    comm.Write('signal_to_rob_05',0)\n",
    "    \n",
    "    \n",
    "    #initialise the depth camera\n",
    "    dc = DepthCamera()\n",
    "    first_frame= True\n",
    "    list=[]\n",
    "    np.array(list)\n",
    "    while first_frame is True:\n",
    "        ret,depth_frame,color_frame= dc.get_frame()\n",
    "        sval=60000\n",
    "        for i in range(50,400):\n",
    "            for j in range(125,700):\n",
    "                if sval>depth_frame[i][j] & depth_frame[i][j]!=0:\n",
    "                    sval=depth_frame[i][j]\n",
    "        if len(list)>10:\n",
    "            list.pop(0)\n",
    "            list.append(sval)\n",
    "        else:\n",
    "            list.append(sval)            \n",
    "            \n",
    "        cv2.imshow(\"color_frame\", color_frame)\n",
    "        #cv2.imshow(\"depth_frame\", depth_frame)\n",
    "    \n",
    "        if cv2.waitKey(1) & len(list)==5: #0xFF == ord('c'):\n",
    "\n",
    "            \n",
    "            list.pop(0)\n",
    "            avg_sval= sum(list)/(len(list))\n",
    "            #ff_d=depth_frame\n",
    "            #ff_c=color_frame\n",
    "            first_frame= False\n",
    "            print(list)\n",
    "            print(len(list))\n",
    "            print(avg_sval)\n",
    "        key= cv2.waitKey(1)\n",
    "        if key == 27:\n",
    "            break\n",
    "    if avg_sval>1050:\n",
    "        diff= avg_sval-1050\n",
    "        move_x = 50\n",
    "        move_y = -70\n",
    "        move_z = diff-1480\n",
    "        \n",
    "    else:\n",
    "        diff=avg_sval-1050\n",
    "        move_x= 50#70\n",
    "        move_y = -70#-18\n",
    "        move_z=-1480+diff\n",
    "\n",
    "    comm.Write('x_axis_frm_pc', move_y)\n",
    "    comm.Write('y_axis_frm_pc', move_x)\n",
    "    comm.Write('z_axis_frm_pc', move_z)\n",
    "    sig_2=1\n",
    "    comm.Write('signal_to_rob_02',sig_2)\n",
    "    print(\"moving robot in z direction :\", move_z)\n",
    "    while True:\n",
    "        #movement of robot in z direction\n",
    "        sig_3=comm.Read(\"signal_to_pc_01\")\n",
    "        if sig_3.Value is True:\n",
    "            break\n",
    "    sig_2=0\n",
    "    comm.Write('signal_to_rob_02',sig_2)\n",
    "    print(\"Moved downwards, now waiting for carton detection\")\n",
    "    first_frame= True\n",
    "    list_2=[]\n",
    "    np.array(list_2)\n",
    "    while first_frame is True:\n",
    "        ret,depth_frame,color_frame= dc.get_frame()\n",
    "        sval=60000\n",
    "        for i in range(50,400):\n",
    "            for j in range(125,700):\n",
    "                if sval>depth_frame[i][j] & depth_frame[i][j]!=0:\n",
    "                    sval=depth_frame[i][j]\n",
    "        if len(list_2)>50:\n",
    "            list_2.pop(0)\n",
    "            list_2.append(sval)\n",
    "        else:\n",
    "            list_2.append(sval)            \n",
    "            \n",
    "        cv2.imshow(\"color_frame\", color_frame)\n",
    "        #cv2.imshow(\"depth_frame\", depth_frame)\n",
    "    \n",
    "        if cv2.waitKey(1) & len(list_2)==15: #0xFF == ord('c'):\n",
    "\n",
    "            \n",
    "            list_2.pop(0)\n",
    "            avg_sval= sum(list_2)/(len(list_2))\n",
    "            ff_d=depth_frame\n",
    "            ff_c=color_frame\n",
    "            print(list_2)\n",
    "            print(len(list_2))\n",
    "            print(avg_sval)\n",
    "\n",
    "            if avg_sval<=1180:\n",
    "                add_value=80\n",
    "            elif avg_sval< 1320:\n",
    "                add_value=80\n",
    "            elif avg_sval< 1480:\n",
    "                add_value= 80\n",
    "            elif avg_sval> 1480:\n",
    "                add_value=100\n",
    "\n",
    "            # Getting the depth sensor's depth scale (see rs-align example for explanation)\n",
    "            #depth_sensor = dc.get_device().first_depth_sensor()\n",
    "            depth_scale = 0.0010000000474974513 #depth_sensor.get_depth_scale()\n",
    "            # We will be removing the background of objects more than\n",
    "            #  clipping_distance_in_meters meters away\n",
    "            clipping_distance_in_meters = (avg_sval+add_value)/1000 #1 meter\n",
    "            clipping_distance = clipping_distance_in_meters / depth_scale\n",
    "\n",
    "            # Remove background - Set pixels further than clipping_distance to grey\n",
    "            grey_color = 0\n",
    "            depth_image_3d = np.dstack((ff_d,ff_d,ff_d)) #depth image is 1 channel, color is 3 channels\n",
    "            bg_removed = np.where((depth_image_3d > clipping_distance) | (depth_image_3d <= 0), grey_color, ff_c)\n",
    "\n",
    "            \"\"\"\n",
    "            Start of:\n",
    "            Reading input image\n",
    "            \"\"\"\n",
    "\n",
    "            # Reading image with OpenCV library\n",
    "            # In this way image is opened already as numpy array\n",
    "            # WARNING! OpenCV by default reads images in BGR format\n",
    "            # Pay attention! If you're using Windows, the path might looks like:\n",
    "            # r'images\\woman-working-in-the-office.jpg'\n",
    "            # or:\n",
    "            # 'images\\\\woman-working-in-the-office.jpg'\n",
    "            #image_BGR = cv2.imread('images/woman-working-in-the-office.jpg') #for linux\n",
    "            #image_BGR = cv2.imread(r'K:\\Vision Tech\\YOLO\\Udemy YOLO\\YOLO-3-OpenCV\\YOLO-3-OpenCV\\images\\test_carton2.jpg') #for windows\n",
    "            image_BGR=bg_removed\n",
    "            # Showing Original Image\n",
    "            # Giving name to the window with Original Image\n",
    "            # And specifying that window is resizable\n",
    "            ###############################cv2.namedWindow('Original Image', cv2.WINDOW_NORMAL)\n",
    "            # Pay attention! 'cv2.imshow' takes images in BGR format\n",
    "            ################################cv2.imshow('Original Image', image_BGR)\n",
    "            # Waiting for any key being pressed\n",
    "            ###################################|cv2.waitKey(0)\n",
    "            # Destroying opened window with name 'Original Image'\n",
    "            ######################################cv2.destroyWindow('Original Image')\n",
    "\n",
    "            # # Check point\n",
    "            # # Showing image shape\n",
    "            # print('Image shape:', image_BGR.shape)  # tuple of (511, 767, 3)\n",
    "\n",
    "            # Getting spatial dimension of input image\n",
    "            h, w = image_BGR.shape[:2]  # Slicing from tuple only first two elements\n",
    "\n",
    "            # # Check point\n",
    "            # # Showing height an width of image\n",
    "            # print('Image height={0} and width={1}'.format(h, w))  # 511 767\n",
    "\n",
    "            \"\"\"\n",
    "            End of: \n",
    "            Reading input image\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Start of:\n",
    "            Getting blob from input image\n",
    "            \"\"\"\n",
    "\n",
    "            # Getting blob from input image\n",
    "            # The 'cv2.dnn.blobFromImage' function returns 4-dimensional blob\n",
    "            # from input image after mean subtraction, normalizing, and RB channels swapping\n",
    "            # Resulted shape has number of images, number of channels, width and height\n",
    "            # E.G.:\n",
    "            # blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size, mean, swapRB=True)\n",
    "            blob = cv2.dnn.blobFromImage(image_BGR, 1 / 255.0, (416, 416),\n",
    "                                        swapRB=True, crop=False)\n",
    "\n",
    "            # # Check point\n",
    "            # print('Image shape:', image_BGR.shape)  # (511, 767, 3)\n",
    "            # print('Blob shape:', blob.shape)  # (1, 3, 416, 416)\n",
    "\n",
    "            # # Check point\n",
    "            # # Showing blob image in OpenCV window\n",
    "            # # Slicing blob image and transposing to make channels come at the end\n",
    "            # blob_to_show = blob[0, :, :, :].transpose(1, 2, 0)\n",
    "            # print(blob_to_show.shape)  # (416, 416, 3)\n",
    "            #\n",
    "            # # Showing Blob Image\n",
    "            # # Giving name to the window with Blob Image\n",
    "            # # And specifying that window is resizable\n",
    "            # cv2.namedWindow('Blob Image', cv2.WINDOW_NORMAL)\n",
    "            # # Pay attention! 'cv2.imshow' takes images in BGR format\n",
    "            # # Consequently, we DO need to convert image from RGB to BGR firstly\n",
    "            # # Because we have our blob in RGB format\n",
    "            # cv2.imshow('Blob Image', cv2.cvtColor(blob_to_show, cv2.COLOR_RGB2BGR))\n",
    "            # # Waiting for any key being pressed\n",
    "            # cv2.waitKey(0)\n",
    "            # # Destroying opened window with name 'Blob Image'\n",
    "            # cv2.destroyWindow('Blob Image')\n",
    "\n",
    "            \"\"\"\n",
    "            End of:\n",
    "            Getting blob from input image\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Start of:\n",
    "            Loading YOLO v3 network\n",
    "            \"\"\"\n",
    "\n",
    "            # Loading COCO class labels from file\n",
    "            # Opening file\n",
    "            # Pay attention! If you're using Windows, yours path might looks like:\n",
    "            # r'yolo-coco-data\\coco.names'\n",
    "            # or:\n",
    "            # 'yolo-coco-data\\\\coco.names'\n",
    "            #with open('yolo-coco-data/coco.names') as f:#for linux\n",
    "            with open(r'K:\\Vision Tech\\YOLO\\Udemy YOLO\\YOLO-3-OpenCV\\YOLO-3-OpenCV\\carton.names') as f: #for windows\n",
    "                # Getting labels reading every line\n",
    "                # and putting them into the list\n",
    "                labels = [line.strip() for line in f]\n",
    "\n",
    "\n",
    "            # # Check point\n",
    "            # print('List with labels names:')\n",
    "            # print(labels)\n",
    "\n",
    "            # Loading trained YOLO v3 Objects Detector\n",
    "            # with the help of 'dnn' library from OpenCV\n",
    "            # Pay attention! If you're using Windows, yours paths might look like:\n",
    "            # r'yolo-coco-data\\yolov3.cfg'\n",
    "            # r'yolo-coco-data\\yolov3.weights'\n",
    "            # or:\n",
    "            # 'yolo-coco-data\\\\yolov3.cfg'\n",
    "            # 'yolo-coco-data\\\\yolov3.weights'\n",
    "            # network = cv2.dnn.readNetFromDarknet('yolo-coco-data/yolov3.cfg',\n",
    "            #                                      'yolo-coco-data/yolov3.weights') #for linux\n",
    "            network = cv2.dnn.readNetFromDarknet(r'K:\\Vision Tech\\YOLO\\Udemy YOLO\\YOLO-3-OpenCV\\YOLO-3-OpenCV\\yolo-coco-data\\yolov4-carton-test.cfg',\n",
    "                                                r'K:\\Vision Tech\\YOLO\\Udemy YOLO\\YOLO-3-OpenCV\\YOLO-3-OpenCV\\yolo-coco-data\\yolov4-carton-train_last.weights') #for windows\n",
    "\n",
    "            # Getting list with names of all layers from YOLO v3 network\n",
    "            layers_names_all = network.getLayerNames()\n",
    "\n",
    "            # # Check point\n",
    "            # print()\n",
    "            # print(layers_names_all)\n",
    "\n",
    "            # Getting only output layers' names that we need from YOLO v3 algorithm\n",
    "            # with function that returns indexes of layers with unconnected outputs\n",
    "            layers_names_output = \\\n",
    "                [layers_names_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\n",
    "\n",
    "            # # Check point\n",
    "            # print()\n",
    "            # print(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']\n",
    "\n",
    "            # Setting minimum probability to eliminate weak predictions\n",
    "            probability_minimum = 0.2\n",
    "\n",
    "            # Setting threshold for filtering weak bounding boxes\n",
    "            # with non-maximum suppression\n",
    "            threshold = 0.3\n",
    "\n",
    "            # Generating colours for representing every detected object\n",
    "            # with function randint(low, high=None, size=None, dtype='l')\n",
    "            colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
    "\n",
    "            # # Check point\n",
    "            # print()\n",
    "            # print(type(colours))  # <class 'numpy.ndarray'>\n",
    "            # print(colours.shape)  # (80, 3)\n",
    "            # print(colours[0])  # [172  10 127]\n",
    "\n",
    "            \"\"\"\n",
    "            End of:\n",
    "            Loading YOLO v3 network\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Start of:\n",
    "            Implementing Forward pass\n",
    "            \"\"\"\n",
    "\n",
    "            # Implementing forward pass with our blob and only through output layers\n",
    "            # Calculating at the same time, needed time for forward pass\n",
    "            network.setInput(blob)  # setting blob as input to the network\n",
    "            start = time.time()\n",
    "            output_from_network = network.forward(layers_names_output)\n",
    "            end = time.time()\n",
    "\n",
    "            # Showing spent time for forward pass\n",
    "            print('Objects Detection took {:.5f} seconds'.format(end - start))\n",
    "\n",
    "            \"\"\"\n",
    "            End of:\n",
    "            Implementing Forward pass\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Start of:\n",
    "            Getting bounding boxes\n",
    "            \"\"\"\n",
    "\n",
    "            # Preparing lists for detected bounding boxes,\n",
    "            # obtained confidences and class's number\n",
    "            bounding_boxes = []\n",
    "            confidences = []\n",
    "            class_numbers = []\n",
    "\n",
    "\n",
    "            # Going through all output layers after feed forward pass\n",
    "            for result in output_from_network:\n",
    "                # Going through all detections from current output layer\n",
    "                for detected_objects in result:\n",
    "                    # Getting 80 classes' probabilities for current detected object\n",
    "                    scores = detected_objects[5:]\n",
    "                    # Getting index of the class with the maximum value of probability\n",
    "                    class_current = np.argmax(scores)\n",
    "                    # Getting value of probability for defined class\n",
    "                    confidence_current = scores[class_current]\n",
    "\n",
    "                    # # Check point\n",
    "                    # # Every 'detected_objects' numpy array has first 4 numbers with\n",
    "                    # # bounding box coordinates and rest 80 with probabilities for every class\n",
    "                    # print(detected_objects.shape)  # (85,)\n",
    "\n",
    "                    # Eliminating weak predictions with minimum probability\n",
    "                    if confidence_current > probability_minimum:\n",
    "                        # Scaling bounding box coordinates to the initial image size\n",
    "                        # YOLO data format keeps coordinates for center of bounding box\n",
    "                        # and its current width and height\n",
    "                        # That is why we can just multiply them elementwise\n",
    "                        # to the width and height\n",
    "                        # of the original image and in this way get coordinates for center\n",
    "                        # of bounding box, its width and height for original image\n",
    "                        box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
    "\n",
    "                        # Now, from YOLO data format, we can get top left corner coordinates\n",
    "                        # that are x_min and y_min\n",
    "                        x_center, y_center, box_width, box_height = box_current\n",
    "                        x_min = int(x_center - (box_width / 2))\n",
    "                        y_min = int(y_center - (box_height / 2))\n",
    "\n",
    "                        # Adding results into prepared lists\n",
    "                        bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
    "                        confidences.append(float(confidence_current))\n",
    "                        class_numbers.append(class_current)\n",
    "\n",
    "            \"\"\"\n",
    "            End of:\n",
    "            Getting bounding boxes\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Start of:\n",
    "            Non-maximum suppression\n",
    "            \"\"\"\n",
    "\n",
    "            # Implementing non-maximum suppression of given bounding boxes\n",
    "            # With this technique we exclude some of bounding boxes if their\n",
    "            # corresponding confidences are low or there is another\n",
    "            # bounding box for this region with higher confidence\n",
    "\n",
    "            # It is needed to make sure that data type of the boxes is 'int'\n",
    "            # and data type of the confidences is 'float'\n",
    "            # https://github.com/opencv/opencv/issues/12789\n",
    "            results = cv2.dnn.NMSBoxes(bounding_boxes, confidences,\n",
    "                                    probability_minimum, threshold)\n",
    "\n",
    "            \"\"\"\n",
    "            End of:\n",
    "            Non-maximum suppression\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Start of:\n",
    "            Drawing bounding boxes and labels\n",
    "            \"\"\"\n",
    "\n",
    "            # Defining counter for detected objects\n",
    "            counter = 1\n",
    "            arr=[]\n",
    "            # Checking if there is at least one detected object after non-maximum suppression\n",
    "            if len(results) > 0:\n",
    "                # Going through indexes of results\n",
    "                for i in results.flatten():\n",
    "                    # Showing labels of the detected objects\n",
    "                    print('Object {0}: {1}'.format(counter, labels[int(class_numbers[i])]))\n",
    "\n",
    "                    # Incrementing counter\n",
    "                    counter += 1\n",
    "\n",
    "                    # Getting current bounding box coordinates,\n",
    "                    # its width and height\n",
    "                    x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
    "                    box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
    "                    cx1= x_min+(box_width/2)\n",
    "                    cy1= y_min+(box_height/2)\n",
    "                    point = (cx1 , cy1)\n",
    "\n",
    "                    distance = avg_sval #ff_d[int(point[1]),int(point[0])] #avg_sval ##nf_depth[240, 320] # [point[1],point[0]]\n",
    "\n",
    "                    cv2.circle(image_BGR, (int(cx1) , int(cy1)), 4, (0, 0, 255))\n",
    "\n",
    "                    dist=distance/10          #input in cm\n",
    "                    each_carton_dist=ff_d[int(point[1]),int(point[0])]\n",
    "                    cv2.putText(image_BGR, \"{}cm\".format(each_carton_dist), (int(point[0]), int(point[1] - 20)), cv2.FONT_HERSHEY_PLAIN, 2, (0, 0, 255), 2)\n",
    "\n",
    "                    cvtfac=0.00325729*distance**(0.914301)-0.144273#0.00304535*distance**(0.921814) -0.17026 #0.00167228*distance -0.00381802\n",
    "                    cx2=(round(cx1*cvtfac,2))\n",
    "                    cy2=(round(cy1*cvtfac,2))\n",
    "\n",
    "\n",
    "                    if (box_width>=box_height):\n",
    "                        cv2.putText(image_BGR,\"H\"+str(cx2)+\",\"+str(cy2), (int(cx1-10), int(cy1-10)), cv2.FONT_HERSHEY_SIMPLEX,0.5, (0, 155, 0), 2)\n",
    "                        orientation=0\n",
    "                    else:\n",
    "                        cv2.putText(image_BGR,\"V\"+str(cx2)+\",\"+str(cy2), (int(cx1-10), int(cy1-10)), cv2.FONT_HERSHEY_SIMPLEX,0.5, (0, 0, 155), 2)\n",
    "                        orientation=900\n",
    "\n",
    "                    #To ROBOT\n",
    "                    robx=(round((cx1-424)*cvtfac,0))\n",
    "                    roby=(round((240-cy1)*cvtfac,0))\n",
    "\n",
    "                    arr.append([robx,roby, move_z-30+distance,0,0 ,orientation])\n",
    "                    # Preparing colour for current bounding box\n",
    "                    # and converting from numpy array to list\n",
    "                    colour_box_current = colours[class_numbers[i]].tolist()\n",
    "\n",
    "                    # # # Check point\n",
    "                    # print(type(colour_box_current))  # <class 'list'>\n",
    "                    # print(colour_box_current)  # [172 , 10, 127]\n",
    "\n",
    "                    # Drawing bounding box on the original image\n",
    "                    cv2.rectangle(image_BGR, (x_min, y_min),\n",
    "                                (x_min + box_width, y_min + box_height),\n",
    "                                colour_box_current, 2)\n",
    "\n",
    "                    # Preparing text with label and confidence for current bounding box\n",
    "                    text_box_current = '{}: {:.4f}'.format(labels[int(class_numbers[i])],\n",
    "                                                        confidences[i])\n",
    "\n",
    "                    # Putting text with label and confidence on the original image\n",
    "                    cv2.putText(image_BGR, text_box_current, (x_min, y_min - 5),\n",
    "                                cv2.FONT_HERSHEY_COMPLEX, 1, colour_box_current, 2)\n",
    "\n",
    "                    # Putting text with center location on the original image\n",
    "                    # cv2.putText(image_BGR, \"{}cm\".format(x_min,box_width), (x_center, y_center),\n",
    "                    #             cv2.FONT_HERSHEY_COMPLEX, 0.7, colour_box_current, 2)\n",
    "            else:\n",
    "                print(\"NO CARTONS DETECTED\")\n",
    "                sig_4=1\n",
    "                #comm.Write('signal_to_rob_04',sig_4)\n",
    "                sig_6=1\n",
    "                comm.Write('signal_to_rob_06',sig_6)\n",
    "                no_carton=1\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "            # Comparing how many objects where before non-maximum suppression\n",
    "            # and left after\n",
    "            print(arr)\n",
    "            print()\n",
    "            print('Total objects been detected:', len(bounding_boxes))\n",
    "            print('Number of objects left after non-maximum suppression:', counter - 1)\n",
    "\n",
    "            \"\"\"\n",
    "            End of:\n",
    "            Drawing bounding boxes and labels\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            # Showing Original Image with Detected Objects\n",
    "            # Giving name to the window with Original Image\n",
    "            # And specifying that window is resizable\n",
    "            cv2.namedWindow('Detections', cv2.WINDOW_NORMAL)\n",
    "            # Pay attention! 'cv2.imshow' takes images in BGR format\n",
    "            cv2.imshow('Detections', image_BGR)\n",
    "            # Waiting for any key being pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                #Destroying opened window with name 'Detections'\n",
    "                cv2.destroyWindow('Detections')\n",
    "            \n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Some comments\n",
    "\n",
    "            With OpenCV function 'cv2.dnn.blobFromImage' we get 4-dimensional\n",
    "            so called 'blob' from input image after mean subtraction,\n",
    "            normalizing, and RB channels swapping. Resulted shape has:\n",
    "            - number of images\n",
    "            - number of channels\n",
    "            - width\n",
    "            - height\n",
    "            E.G.: blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size, mean, swapRB=True)\n",
    "            \"\"\"\n",
    "\n",
    "            #comm.Write('signal_to_rob_05',0)\n",
    "\n",
    "            arr=Sort(arr)\n",
    "            print(arr)\n",
    "            for i in range(len(arr)):\n",
    "                comm =PLC()\n",
    "                comm.IPAddress = '192.168.255.2'\n",
    "                n=comm.Read('pulse_to_pc')\n",
    "                print(n.Value)\n",
    "                real_x=arr[i][0]\n",
    "                real_y=arr[i][1]\n",
    "                real_z=arr[i][2]\n",
    "                real_rx=arr[i][3]\n",
    "                real_ry=arr[i][4]\n",
    "                real_rz=arr[i][5]\n",
    "                print(\"Grab item from x:\" +str(real_y)+\", y:\"+str(real_x)+\", z:\"+str(real_z)+\", Rz:\"+str(real_rz))\n",
    "                comm.IPAddress = '192.168.255.2'\n",
    "                comm.Write('x_axis_frm_pc', real_y)\n",
    "                comm.Write('y_axis_frm_pc', real_x)\n",
    "                comm.Write('z_axis_frm_pc', real_z)\n",
    "                comm.Write('r_x_frm_pc', real_rx)\n",
    "                comm.Write('r_y_frm_pc', real_ry)\n",
    "                comm.Write('r_z_frm_pc', real_rz)\n",
    "                sig_4=1\n",
    "                comm.Write('signal_to_rob_04',sig_4)#\n",
    "                comm.Close()\n",
    "                #time.sleep(20)\n",
    "                while n.Value is False:\n",
    "                    #print(n.Value)\n",
    "                    n=comm.Read('pulse_to_pc')# should get pulse of '1'\n",
    "                    #print(n.Value)\n",
    "                    if keyboard.is_pressed('p'):        # press 0 to exit the while loop\n",
    "                        arr=[]\n",
    "                        break\n",
    "                if not arr:\n",
    "                    break\n",
    "            sig_5=1\n",
    "            comm.Write('signal_to_rob_05',sig_5)\n",
    "            first_frame=False\n",
    "    \n",
    "\n",
    "\n",
    "    print(\"exited inner loop\")\n",
    "    if no_carton==1 or keyboard.is_pressed('q'):\n",
    "        print(\"Exiting the code\")\n",
    "        #sig_6=1\n",
    "        #comm.Write('signal_to_rob_06',sig_6)\n",
    "        cv2.destroyAllWindows()\n",
    "        dc.release()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(TagName=signal_to_rob_05, Value=1, Status=Success)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm.Write('signal_to_rob_05',1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a=comm.Read('signal_to_rob_05')\n",
    "print(a.Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opencv-python==4.5.3.56\n",
      "  Downloading opencv_python-4.5.3.56-cp39-cp39-win_amd64.whl (34.9 MB)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\keyur.borad\\appdata\\roaming\\python\\python39\\site-packages (from opencv-python==4.5.3.56) (1.21.5)\n",
      "Installing collected packages: opencv-python\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.5.5.62\n",
      "    Uninstalling opencv-python-4.5.5.62:\n",
      "      Successfully uninstalled opencv-python-4.5.5.62\n",
      "Successfully installed opencv-python-4.5.3.56\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\keyur.borad\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\keyur.borad\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\keyur.borad\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\keyur.borad\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\keyur.borad\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\keyur.borad\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\keyur.borad\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\keyur.borad\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: You are using pip version 21.1.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade opencv-python==4.5.3.56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
